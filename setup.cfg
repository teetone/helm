[metadata]
name = crfm-helm
version = 0.2.3
author = Stanford CRFM
author_email = contact-crfm@stanford.edu
description = Benchmark for language models
long_description = Benchmark for language models
keywords = language models benchmarking
license = Apache License 2.0
classifiers =
    Programming Language :: Python :: 3 :: Only
    Programming Language :: Python :: 3.8
    License :: OSI Approved :: Apache Software License
url = https://github.com/stanford-crfm/helm

[options]
python_requires = ~=3.8
package_dir =
    =src
packages = find:
zip_safe = False
include_package_data = True

install_requires=
    # Common
    cattrs~=22.2.0
    dacite~=1.6.0
    importlib-resources~=5.10.0
    Mako~=1.2.3
    numpy~=1.23.3
    pyhocon~=0.3.59
    retrying~=1.3.4
    simple-slurm~=0.2.6  # For slurm_jobs
    spacy~=3.5.3
    tqdm~=4.64.1
    zstandard~=0.18.0
    # sqlitedict==2.0.0 is slow! https://github.com/RaRe-Technologies/sqlitedict/issues/152
    # Keep sqlitedict version at 1.7.0.
    sqlitedict~=1.7.0
    # TODO: Remove these from common
    protobuf~=3.20.2  # Can't use 4.21.0 due to backward incompatibility
    pymongo~=4.2.0

    # Basic Scenarios
    datasets~=2.5.2
    pyarrow~=11.0.0  # Pinned transitive dependency for datasets; workaround for #1026
    jsonlines~=3.1.0  # Not really needed

    # Basic metrics
    nltk~=3.7
    pyext~=0.7
    rouge-score~=0.1.2
    scipy~=1.9.1
    uncertainty-calibration~=0.1.3
    # Work around https://github.com/p-lambda/verified_calibration/issues/11
    # TODO: Remove after this issue is resolved
    scikit-learn~=1.1.2

    # Server Extras
    bottle~=0.12.23
    gunicorn~=20.1.0

    # Scenario Extras
    gdown~=4.4.0  # For opinions_qa_scenario
    sympy~=1.11.1  # For numeracy_scenario
    xlrd~=2.0.1  # For ice_scenario: used by pandas.read_excel

    # Model Extras
    aleph-alpha-client~=2.14.0
    anthropic~=0.2.5
    icetk~=0.0.4  # for ice_tokenizer_client
    openai~=0.27.8
    revChatGPT~=0.1.1
    sentencepiece~=0.1.97  # For palmyra_client and yalm_tokenizer
    tiktoken~=0.3.3  # for openai_client
    tokenizers~=0.13.3  # for aleph_alpha_client
    websocket-client~=1.3.2  # For Anthropic (Legacy stanford-online-all-v4-s3)

    # Models and Metrics Extras
    transformers~=4.28.1  # For anthropic_client, huggingface_client, huggingface_tokenizer, test_openai_token_cost_estimator, model_summac (via summarization_metrics)
    # TODO: Upgrade torch
    torch>=1.12.1,<3.0.0  # For huggingface_client, yalm_tokenizer, model_summac (via summarization_metrics)
    torchvision>=0.13.1,<3.0.0  # For huggingface_client, yalm_tokenizer, model_summac (via summarization_metrics)

    # Metrics Extras
    google-api-python-client~=2.64.0  # For perspective_api_client via toxicity_metrics
    numba~=0.56.4  # For copyright_metrics
    pytrec_eval~=0.5  # For ranking_metrics
    sacrebleu~=2.2.1  # For disinformation_metrics, machine_translation_metrics
    summ-eval~=0.892  # For summarization_metrics

    # Human Evaluation Extras
    scaleapi~=2.13.0
    surge-api~=1.1.0

    # Plots Extras
    colorcet~=3.0.1
    matplotlib~=3.6.0
    seaborn~=0.11.0

[options.extras_require]
heim =
    # HEIM metrics
    clip-anytorch~=2.5.0
    NudeNet~=2.0.9
    opencv-python~=4.7.0.68
    Pillow~=9.4.0
    pytorch-fid~=0.3.0
    timm~=0.6.12
    torch-fidelity~=0.3.0
    torchmetrics~=0.11.1

    accelerate~=0.17.0
    antlr4-python3-runtime~=4.9.3
    appnope~=0.1.3
    asttokens~=2.2.1
    astunparse~=1.6.3
    autokeras~=1.0.20
    backcall~=0.2.0
    decorator~=5.1.1
    diffusers~=0.13.1
    einops~=0.6.0
    executing~=1.2.0
    flatbuffers~=1.12
    flax~=0.6.3
    ftfy~=6.1.1
    gast~=0.4.0
    google-auth-oauthlib~=0.4.6
    google-cloud-core~=2.3.3
    google-cloud-storage~=2.9.0
    google-cloud-translate~=3.11.2
    google-pasta~=0.2.0
    grpcio~=1.51.1
    grpcio-status~=1.48.2
    h5py~=3.8.0
    ipython~=8.9.0
    jax~=0.3.25
    jaxlib~=0.3.25
    jedi~=0.18.2
    keras~=2.9.0
    Keras-Preprocessing~=1.1.2
    keras-tuner~=1.2.0
    kt-legacy~=1.0.4
    libclang~=15.0.6.1
    libcst~=0.4.9
    lpips~=0.1.4
    Markdown~=3.4.1
    matplotlib-inline~=0.1.6
    multilingual-clip~=1.0.10
    oauthlib~=3.2.2
    omegaconf~=2.3.0
    opencv-python~=4.7.0.68
    opt-einsum~=3.3.0
    parso~=0.8.3
    pexpect~=4.8.0
    pickleshare~=0.7.5
    prompt-toolkit~=3.0.36
    proto-plus~=1.22.2
    ptyprocess~=0.7.0
    pure-eval~=0.2.2
    pyDeprecate~=0.3.2
    Pygments~=2.14.0
    pytorch-fid~=0.3.0
    pytorch-lightning~=2.0.5
    requests-oauthlib~=1.3.1
    stack-data~=0.6.2
    SwissArmyTransformer~=0.2.1
    tensorboard~=2.9.0
    tensorboard-data-server~=0.6.1
    tensorboard-plugin-wit~=1.8.1
    tensorflow~=2.9.0
    tensorflow-estimator~=2.9.0
    tensorflow-io-gcs-filesystem~=0.30.0
    termcolor~=2.2.0
    timm~=0.6.12
    torch-fidelity~=0.3.0
    torchmetrics~=0.11.1
    traitlets~=5.9.0
    typing-inspect~=0.8.0
    Unidecode~=1.3.6
    wandb~=0.13.11
    wcwidth~=0.2.6
    Werkzeug~=2.2.2
    wrapt~=1.14.1

[options.entry_points]
console_scripts = 
    helm-run = helm.benchmark.run:main
    helm-summarize = helm.benchmark.presentation.summarize:main
    helm-server = helm.benchmark.server:main
    helm-create-plots = helm.benchmark.presentation.create_plots:main
    crfm-proxy-server = helm.proxy.server:main
    crfm-proxy-cli = helm.proxy.cli:main

[options.packages.find]
where = src
exclude =
    tests*

# Settings for Flake8: Tool For Style Guide Enforcement
[flake8]
max-line-length = 120
exclude =
    venv/*
    src/helm/proxy/clients/image_generation/dalle_mini/*
    src/helm/proxy/clients/image_generation/mindalle/*

# Ignore completely:
# E203 - White space before ':', (conflicts with black)
# E231 - Missing whitespace after ',', ';', or ':'
# E731 - do not assign a lambda expression, use a def
# W503 - line break before binary operator, (conflicts with black)
# W605 - invalid escape sequence '\', (causes failures)
ignore = E203,E231,E731,W503,W605

# Settings for Mypy: static type checker for Python 3
[mypy]
ignore_missing_imports = True
exclude = dalle_mini|mindalle

[tool:pytest]
addopts =
    # By default, we don't test models because doing so will
    # make real requests and spend real money
    -m 'not models'
markers =
    # Marker for tests that make real model requests
    models